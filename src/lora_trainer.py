"""
Qwen 0.6B LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

PEFT (Parameter-Efficient Fine-Tuning) ã‚’ä½¿ç”¨ã—ãŸLoRAå­¦ç¿’
"""

import os
import json
import torch
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType,
)
from datasets import load_dataset
import numpy as np
from dotenv import load_dotenv

# .envã¨.env.secretsã®ä¸¡æ–¹ã‚’èª­ã¿è¾¼ã¿
load_dotenv(override=True)  # .envï¼ˆå¸¸ã«ä¸Šæ›¸ãï¼‰
load_dotenv('.env.secrets', override=True)  # .env.secrets (å„ªå…ˆ)


@dataclass
class LoRATrainingConfig:
    """LoRAå­¦ç¿’ã®è¨­å®š"""
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    base_model: str = os.getenv("BASE_MODEL", "Qwen/Qwen3-0.6B")
    max_length: int = int(os.getenv("MAX_LENGTH", "512"))
    
    # LoRAè¨­å®š
    lora_r: int = int(os.getenv("LORA_R", "16"))  # 8â†’16ã«å¢—åŠ ï¼ˆ4å€ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
    lora_alpha: int = int(os.getenv("LORA_ALPHA", "32"))  # rÃ—2ã‚’ç¶­æŒ
    lora_dropout: float = float(os.getenv("LORA_DROPOUT", "0.05"))  # éå­¦ç¿’ã‚’æŠ‘åˆ¶ã™ã‚‹ãŸã‚æ¸›å°‘
    target_modules: list = None
    
    # å­¦ç¿’è¨­å®š
    num_epochs: int = int(os.getenv("NUM_EPOCHS", "100"))
    num_epochs_followup: int = int(os.getenv("NUM_EPOCHS_FOLLOWUP", "30"))
    batch_size: int = int(os.getenv("BATCH_SIZE", "4"))
    learning_rate: float = float(os.getenv("LEARNING_RATE", "2e-4"))
    warmup_steps: int = 100
    logging_steps: int = 1
    save_steps: int = 100
    
    # å‡ºåŠ›è¨­å®š
    output_dir: str = "checkpoints"
    
    def __post_init__(self):
        if self.target_modules is None:
            # Qwen3 (Qwen2.5ãƒ™ãƒ¼ã‚¹) ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
            # Attentionã®ã¿ã®å ´åˆ: ["q_proj", "k_proj", "v_proj", "o_proj"]
            # ã‚ˆã‚Šå¤šãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã™ã‚‹å ´åˆã¯MLPã‚‚å«ã‚ã‚‹
            self.target_modules = [
                "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
                "gate_proj", "up_proj", "down_proj"  # MLP (SwiGLU FFN)
            ]


class MedicalLoRATrainer:
    """åŒ»ç™‚LLMã®LoRAãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼"""
    
    def __init__(self, config: LoRATrainingConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.peft_model = None
        
    def setup_model(self, previous_checkpoint: str = None):
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        
        Args:
            previous_checkpoint: å‰å›ã®ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ï¼ˆç¶™ç¶šå­¦ç¿’ã®å ´åˆï¼‰
        """
        if previous_checkpoint:
            print(f"\nğŸ”§ å‰å›ã®ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç¶™ç¶šå­¦ç¿’: {previous_checkpoint}")
        else:
            print(f"\nğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ–°è¦å­¦ç¿’: {self.config.base_model}")
        
        # Hugging Face Tokenã®è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰ï¼‰
        hf_token = os.getenv("HF_TOKEN", None)
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿ï¼ˆå¸¸ã«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ï¼‰
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.base_model,
            trust_remote_code=True,
            padding_side="right",
            token=hf_token
        )
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        if previous_checkpoint:
            # å‰å›ã®ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ–°ã—ã„ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦èª­ã¿è¾¼ã‚€
            print("  ğŸ“¥ å‰å›ã®ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            self.model = AutoModelForCausalLM.from_pretrained(
                previous_checkpoint,
                dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                token=hf_token
            )
            print("âœ… ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            print("   ğŸ’¡ ã“ã®ãƒ¢ãƒ‡ãƒ«ã«æ–°ã—ã„LoRAãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’è¿½åŠ ã—ã¦å­¦ç¿’ã—ã¾ã™")
        else:
            # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ–°è¦å­¦ç¿’
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.base_model,
                dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                token=hf_token
            )
            print("âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        
        # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æœ‰åŠ¹åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
        self.model.gradient_checkpointing_enable()
        self.model = prepare_model_for_kbit_training(self.model)
        
    def setup_lora(self):
        """LoRAè¨­å®šã®é©ç”¨"""
        print("\nğŸ”§ LoRAè¨­å®šã‚’é©ç”¨ä¸­...")
        
        lora_config = LoraConfig(
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            target_modules=self.config.target_modules,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ™ãƒ¼ã‚¹ã¾ãŸã¯ãƒãƒ¼ã‚¸æ¸ˆã¿ï¼‰ã«æ–°ã—ã„LoRAãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’è¿½åŠ 
        self.peft_model = get_peft_model(self.model, lora_config)
        self.peft_model.print_trainable_parameters()
        
        print("âœ… LoRAè¨­å®šå®Œäº†")
        
    def prepare_dataset(self, data_path: str):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™"""
        print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™: {data_path}")
        
        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        dataset = load_dataset("json", data_files=data_path, split="train")
        
        def format_instruction(example):
            """å‘½ä»¤å½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
            prompt = f"""ä»¥ä¸‹ã¯ã€ã‚¿ã‚¹ã‚¯ã‚’èª¬æ˜ã™ã‚‹æŒ‡ç¤ºã§ã™ã€‚è¦æ±‚ã‚’é©åˆ‡ã«æº€ãŸã™å¿œç­”ã‚’æ›¸ããªã•ã„ã€‚

### æŒ‡ç¤º:
{example['instruction']}

### å¿œç­”:
{example['output']}"""
            return {"text": prompt}
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_dataset = dataset.map(format_instruction)
        
        def tokenize_function(examples):
            """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–"""
            return self.tokenizer(
                examples["text"],
                truncation=True,
                max_length=self.config.max_length,
                padding="max_length",
            )
        
        tokenized_dataset = formatted_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=formatted_dataset.column_names,
        )
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†: {len(tokenized_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
        return tokenized_dataset
    
    def train(self, train_dataset, iteration: int = 1, previous_checkpoint: str = None):
        """LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ
        
        å‰å›ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã¯ç¶™ç¶šå­¦ç¿’ã‚’è¡Œã„ã¾ã™ã€‚
        ã“ã‚Œã«ã‚ˆã‚Šã€æ­£ç­”ã—ã¦ã„ãŸå•é¡Œã‚’å¿˜ã‚Œãšã€é–“é•ãˆãŸå•é¡Œã ã‘ã‚’è¿½åŠ å­¦ç¿’ã§ãã¾ã™ã€‚
        
        Args:
            train_dataset: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            iteration: ç¾åœ¨ã®iterationç•ªå·
            previous_checkpoint: å‰å›ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹ï¼ˆç¶™ç¶šå­¦ç¿’ã®å ´åˆï¼‰
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
        base_output_dir = os.path.abspath(self.config.output_dir)
        output_dir = os.path.join(
            base_output_dir,
            f"iteration_{iteration}_{timestamp}"
        )
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’äº‹å‰ã«ä½œæˆï¼ˆè¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚å«ã‚ã¦ï¼‰
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"\nğŸš€ å­¦ç¿’é–‹å§‹ (Iteration {iteration})")
        print(f"   å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
        if previous_checkpoint:
            print(f"   å‰å›ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ç¶™ç¶šå­¦ç¿’: {previous_checkpoint}")
        else:
            print("   ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ–°è¦ã«å­¦ç¿’ã—ã¾ã™")
        
        # iterationã«å¿œã˜ã¦ã‚¨ãƒãƒƒã‚¯æ•°ã‚’æ±ºå®šï¼ˆiteration 0ã®ã¿100ã‚¨ãƒãƒƒã‚¯ï¼‰
        epochs = self.config.num_epochs if iteration == 0 else self.config.num_epochs_followup
        print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {epochs}")
        
        # å­¦ç¿’å¼•æ•°ã®è¨­å®š
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=epochs,
            per_device_train_batch_size=self.config.batch_size,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            save_total_limit=2,
            fp16=True,
            optim="adamw_torch",
            lr_scheduler_type="cosine",
            report_to="tensorboard",
            logging_dir=f"{output_dir}/logs",
        )
        
        # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ã®è¨­å®š
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
        trainer = Trainer(
            model=self.peft_model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=data_collator,
        )
        
        # å­¦ç¿’ã®å®Ÿè¡Œ
        print("\nâ³ å­¦ç¿’ä¸­...")
        trainer.train()
        
        # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ»åˆ†æç”¨ï¼‰
        lora_adapter_dir = os.path.join(output_dir, "lora_adapter")
        trainer.save_model(lora_adapter_dir)
        print(f"\nğŸ’¾ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ä¿å­˜: {lora_adapter_dir}")
        
        # LoRAã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸ã—ã¦ä¿å­˜ï¼ˆæ¬¡å›ã®ç¶™ç¶šå­¦ç¿’ãƒ»è©•ä¾¡ç”¨ï¼‰
        print("\nğŸ”€ LoRAã‚’ãƒãƒ¼ã‚¸ã—ã¦çµ±åˆãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä¿å­˜ä¸­...")
        merged_model = self.peft_model.merge_and_unload()
        
        final_output_dir = os.path.join(output_dir, "final")
        merged_model.save_pretrained(final_output_dir)
        self.tokenizer.save_pretrained(final_output_dir)
        
        print(f"âœ… ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {final_output_dir}")
        print("   ğŸ’¡ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ¬¡å›ã®iteration or è©•ä¾¡ã§ç›´æ¥èª­ã¿è¾¼ã‚ã¾ã™ï¼ˆLoRAä¸è¦ï¼‰")
        
        # å­¦ç¿’çµ±è¨ˆã®ä¿å­˜
        stats = {
            "iteration": iteration,
            "timestamp": timestamp,
            "config": {
                "lora_r": self.config.lora_r,
                "lora_alpha": self.config.lora_alpha,
                "learning_rate": self.config.learning_rate,
                "num_epochs": epochs,
                "batch_size": self.config.batch_size,
            },
            "output_dir": final_output_dir,
            "lora_adapter_dir": lora_adapter_dir,
        }
        
        with open(os.path.join(output_dir, "training_stats.json"), "w") as f:
            json.dump(stats, f, indent=2)
        
        print("\nâœ… å­¦ç¿’å®Œäº†!")
        print(f"   ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: {final_output_dir}")
        print(f"   LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼: {lora_adapter_dir}")
        
        return final_output_dir
    
    def generate_response(self, prompt: str, max_new_tokens: int = 256) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®å¿œç­”ç”Ÿæˆï¼ˆæ¨è«–ï¼‰"""
        formatted_prompt = f"""ä»¥ä¸‹ã¯ã€ã‚¿ã‚¹ã‚¯ã‚’èª¬æ˜ã™ã‚‹æŒ‡ç¤ºã§ã™ã€‚è¦æ±‚ã‚’é©åˆ‡ã«æº€ãŸã™å¿œç­”ã‚’æ›¸ããªã•ã„ã€‚

### æŒ‡ç¤º:
{prompt}

### å¿œç­”:
"""
        
        inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.peft_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
        if "### å¿œç­”:" in response:
            response = response.split("### å¿œç­”:")[-1].strip()
        
        return response


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("=" * 70)
    print("Qwen 0.6B LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
    print("=" * 70)
    
    # è¨­å®šã®åˆæœŸåŒ–
    config = LoRATrainingConfig()
    
    # è¨­å®šå†…å®¹ã®è¡¨ç¤º
    print("\nğŸ“‹ å­¦ç¿’è¨­å®š:")
    print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {config.num_epochs}")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}")
    print(f"   å­¦ç¿’ç‡: {config.learning_rate}")
    print(f"   LoRA rank: {config.lora_r}")
    print(f"   LoRA alpha: {config.lora_alpha}")
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
    trainer = MedicalLoRATrainer(config)
    trainer.setup_model()
    trainer.setup_lora()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
    train_dataset = trainer.prepare_dataset("data/training_data.jsonl")
    
    # å­¦ç¿’ã®å®Ÿè¡Œ
    checkpoint_path = trainer.train(train_dataset, iteration=1)
    
    # ãƒ†ã‚¹ãƒˆæ¨è«–
    print("\n" + "=" * 70)
    print("ãƒ†ã‚¹ãƒˆæ¨è«–")
    print("=" * 70)
    
    test_prompt = "ä»¥ä¸‹ã®ç—‡çŠ¶ã‚’æŒã¤æˆäººæ‚£è€…ã«å¯¾ã—ã¦ã€ä¸€èˆ¬çš„ã«ç”¨ã„ã‚‰ã‚Œã‚‹è–¬å‰¤å€™è£œã‚’è¤‡æ•°æŒ™ã’ã¦ãã ã•ã„ã€‚\n\nç—‡çŠ¶: é ­ç—›ã¨ç™ºç†±"
    response = trainer.generate_response(test_prompt)
    
    print(f"\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {test_prompt}")
    print(f"\nå¿œç­”:\n{response}")


if __name__ == "__main__":
    main()
