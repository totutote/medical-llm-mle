"""
åŒ»ç™‚LLM LoRAåå¾©å­¦ç¿’ãƒ»è©•ä¾¡ãƒ«ãƒ¼ãƒ—ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼

ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ â†’ LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° â†’ è©•ä¾¡ ã®ã‚µã‚¤ã‚¯ãƒ«ã‚’ç®¡ç†
"""

import os
import json
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Optional, List, Dict

from data_generator import create_training_data, create_evaluation_data, load_evaluation_data
from lora_trainer import MedicalLoRATrainer, LoRATrainingConfig
from evaluator import MedicalLLMEvaluator

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


class IterativeTrainingLoop:
    """åå¾©å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®ç®¡ç†"""
    
    def __init__(
        self,
        base_model_name: str,
        num_iterations: int = 2,
        output_dir: str = "results",
        data_dir: str = "data",
        num_samples_per_question: int = 3,  # Nå›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆä»•æ§˜æ›¸æº–æ‹ ï¼‰
        enable_baseline: bool = False,  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’æœ‰åŠ¹åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç„¡åŠ¹ï¼‰
        rehearsal_ratio: float = 1.0  # æˆåŠŸä¾‹ãƒªãƒãƒ¼ã‚µãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡ï¼ˆèª¤ç­”æ•°ã«å¯¾ã™ã‚‹å€ç‡ï¼‰
    ):
        self.base_model_name = base_model_name
        self.num_iterations = num_iterations
        self.output_dir = Path(output_dir)
        self.data_dir = Path(data_dir)
        self.num_samples_per_question = num_samples_per_question
        self.enable_baseline = enable_baseline
        self.rehearsal_ratio = rehearsal_ratio
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        self.experiment_id = datetime.now(timezone(timedelta(hours=9))).strftime("%Y%m%d_%H%M%S")
        self.iteration_results = []
        
    def setup_data(self, num_train_samples: int = None, num_eval_samples: int = 15):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆä»•æ§˜æ›¸æº–æ‹ : 15ä»¶ï¼‰"""
        print("\n" + "=" * 70)
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ")
        print("=" * 70)
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆNoneã®å ´åˆã¯å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
        train_path = create_training_data(
            output_dir=str(self.data_dir),
            num_samples=num_train_samples
        )
        
        # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        eval_path = create_evaluation_data(
            output_dir=str(self.data_dir),
            num_samples=num_eval_samples
        )
        
        return train_path, eval_path
    
    def evaluate_model(
        self,
        model_name_or_path: str,
        eval_data: List[Dict],
        iteration: int,
        is_baseline: bool = False
    ) -> Dict:
        """ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        
        iteration_name = "baseline" if is_baseline else f"iteration_{iteration}"
        print("\n" + "=" * 70)
        print(f"ğŸ“ˆ è©•ä¾¡: {iteration_name}")
        print("=" * 70)
        
        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
        print(f"\nğŸ”§ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_name_or_path}")
        
        # Hugging Face Tokenã®è¨­å®š
        hf_token = os.getenv("HF_TOKEN", None)
        
        tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_name,
            trust_remote_code=True,
            token=hf_token
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ç›´æ¥èª­ã¿è¾¼ã‚€ï¼ˆLoRAä¸è¦ï¼‰
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚‚iterationå¾Œã®ãƒ¢ãƒ‡ãƒ«ã‚‚åŒã˜æ–¹æ³•ã§èª­ã¿è¾¼ã‚ã‚‹
        model = AutoModelForCausalLM.from_pretrained(
            model_name_or_path,
            dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            token=hf_token
        )
        
        # æ¨è«–é–¢æ•°ã®å®šç¾©
        def generate_fn(prompt: str) -> str:
            formatted_prompt = f"""ä»¥ä¸‹ã¯ã€ã‚¿ã‚¹ã‚¯ã‚’èª¬æ˜ã™ã‚‹æŒ‡ç¤ºã§ã™ã€‚è¦æ±‚ã‚’é©åˆ‡ã«æº€ãŸã™å¿œç­”ã‚’æ›¸ããªã•ã„ã€‚

### æŒ‡ç¤º:
{prompt}

### å¿œç­”:
"""
            inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=150,  # 256ã®2/3ã«çŸ­ç¸®ã—ã¦å†—é•·ãªå‡ºåŠ›ã‚’é˜²ã
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    repetition_penalty=1.2,  # ç¹°ã‚Šè¿”ã—ã‚’é˜²ã
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            if "### å¿œç­”:" in response:
                response = response.split("### å¿œç­”:")[-1].strip()
            
            return response
        
        # è©•ä¾¡å®Ÿè¡Œï¼ˆNå›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        evaluator = MedicalLLMEvaluator()
        
        all_results = []
        print(f"\nğŸ“Š å„è³ªå•ã«å¯¾ã—ã¦{self.num_samples_per_question}å›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ")
        
        for question_idx, test_case in enumerate(eval_data, 1):
            print(f"\nè³ªå• {question_idx}/{len(eval_data)}: {test_case['symptom'][:50]}...")
            
            for sample_idx in range(self.num_samples_per_question):
                symptom = test_case["symptom"]
                expected = test_case["expected_medications"]
                
                # æœ€åˆã¨æœ€å¾Œã®ã‚µãƒ³ãƒ—ãƒ«ã¯è©³ç´°è¡¨ç¤º
                show_details = (sample_idx == 0 or sample_idx == self.num_samples_per_question - 1)
                
                # ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¿œç­”ã‚’ç”Ÿæˆ(è»½é‡ãƒ¢ãƒ‡ãƒ«å‘ã‘ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå½¢å¼)
                if show_details:
                    print(f"\n  [{sample_idx + 1}/{self.num_samples_per_question}] ãƒ¢ãƒ‡ãƒ«æ¨è«–ä¸­...")
                else:
                    print(f"  [{sample_idx + 1}/{self.num_samples_per_question}] ãƒ¢ãƒ‡ãƒ«æ¨è«–ä¸­...", end="", flush=True)
                from data_generator import format_instruction
                prompt = format_instruction(symptom)
                
                # ãƒ¢ãƒ‡ãƒ«æ¨è«–æ™‚é–“ã®è¨ˆæ¸¬
                import time
                inference_start_time = time.time()
                model_response = generate_fn(prompt)
                inference_elapsed_time = time.time() - inference_start_time
                
                if show_details:
                    print(f"  â±ï¸  ãƒ¢ãƒ‡ãƒ«æ¨è«–æ™‚é–“: {inference_elapsed_time:.2f}ç§’")
                else:
                    print(f" ({inference_elapsed_time:.2f}ç§’)", end="", flush=True)
                
                if show_details:
                    print(f"  ğŸ“ ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›:\n{'-' * 60}")
                    print(f"{model_response}")
                    print(f"{'-' * 60}")
                
                # è©•ä¾¡ã‚’å®Ÿè¡Œ
                if show_details:
                    print(f"  ğŸ¤– ChatGPTè©•ä¾¡ä¸­...")
                else:
                    print(" â†’ ChatGPTè©•ä¾¡ä¸­...", end="", flush=True)
                
                evaluation = evaluator.evaluate_single(symptom, model_response, expected)
                all_results.append(evaluation)
                
                if show_details:
                    # JSONå½¢å¼ã§è©•ä¾¡çµæœã‚’è¡¨ç¤º
                    from dataclasses import asdict
                    eval_dict = asdict(evaluation)
                    print(f"  âœ… è©•ä¾¡çµæœ (JSON):")
                    print(json.dumps(eval_dict, ensure_ascii=False, indent=2))
                else:
                    print(f" âœ“ (ãƒ©ãƒ™ãƒ«: {evaluation.overall_label})")
                
                if (sample_idx + 1) % 10 == 0:
                    print(f"  ğŸ“Š é€²æ—: {sample_idx + 1}/{self.num_samples_per_question} å®Œäº†")
        
        results = all_results
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        metrics = evaluator.calculate_metrics(results)
        
        # çµæœä¿å­˜
        result_path = self.output_dir / f"{iteration_name}_evaluation.json"
        evaluator.save_results(results, str(result_path))
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        evaluator.print_summary(results)
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del model
        del tokenizer
        torch.cuda.empty_cache()
        
        return {
            "iteration": iteration,
            "is_baseline": is_baseline,
            "model_path": model_name_or_path,
            "metrics": metrics,
            "result_path": str(result_path),
            "timestamp": datetime.now(timezone(timedelta(hours=9))).isoformat()
        }
    
    def extract_incorrect_cases(self, results: List) -> List[Dict]:
        """é–“é•ãˆãŸã‚±ãƒ¼ã‚¹ã‚’æŠ½å‡ºã—ã¦æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ï¼ˆSAMPLE_MEDICAL_DATAã‹ã‚‰æ­£ã—ã„ç­”ãˆã‚’å–å¾—ï¼‰"""
        incorrect_samples = []
        
        for result in results:
            # overall_labelãŒincorrect/unsafe/partially_correctã‚’æŠ½å‡º
            if result.overall_label in ["incorrect", "unsafe", "partially_correct"]:
                # ç—‡çŠ¶ã‚’æŠ½å‡º
                symptom = result.question
                
                # SAMPLE_MEDICAL_DATAã‹ã‚‰æ­£ã—ã„ç­”ãˆã‚’å–å¾—
                from data_generator import find_correct_medication_from_sample_data, format_instruction
                correct_medications = find_correct_medication_from_sample_data(symptom)
                
                if correct_medications:
                    instruction = format_instruction(symptom)
                    
                    # å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«åˆã‚ã›ã‚‹: "è–¬å‰¤å - ç†ç”±\n2. è–¬å‰¤å - ç†ç”±\n3. è–¬å‰¤å - ç†ç”±"
                    formatted_output = self._format_medications_from_json(correct_medications)
                    
                    incorrect_samples.append({
                        "instruction": instruction,
                        "input": "",
                        "output": formatted_output,
                        "symptom": symptom  # ãƒãƒƒãƒãƒ³ã‚°ç”¨
                    })
                else:
                    print(f"  âš ï¸  SAMPLE_MEDICAL_DATAã‹ã‚‰æ­£ã—ã„ç­”ãˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {symptom[:50]}...")
        
        print(f"  æŠ½å‡ºã•ã‚ŒãŸèª¤ç­”ãƒ»ä¸å®‰å…¨ã‚±ãƒ¼ã‚¹: {len(incorrect_samples)}ä»¶")
        return incorrect_samples
    
    def _format_medications_from_json(self, medications: List[Dict]) -> str:
        """JSONé…åˆ—å½¢å¼ã®è–¬å‰¤ãƒªã‚¹ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«æ•´å½¢
        
        Args:
            medications: [{"name": "è–¬å‰¤å", "reason": "ç†ç”±"}, ...] å½¢å¼ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            "è–¬å‰¤å1 - ç†ç”±1\n2. è–¬å‰¤å2 - ç†ç”±2\n3. è–¬å‰¤å3 - ç†ç”±3" å½¢å¼ã®æ–‡å­—åˆ—
        """
        if not medications or len(medications) == 0:
            return ""
        
        # æœ€å¤§3ã¤ã¾ã§
        meds = medications[:3]
        lines = []
        
        for i, med in enumerate(meds, start=1):
            name = med.get("name", "")
            reason = med.get("reason", "")
            
            if i == 1:
                # æœ€åˆã¯ "1. " ãªã—ï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«åˆã‚ã›ã‚‹ï¼‰
                lines.append(f"{name} - {reason}")
            else:
                lines.append(f"{i}. {name} - {reason}")
        
        return "\n".join(lines)
    
    def extract_correct_cases(self, results: List, num_samples: int) -> List[Dict]:
        """æ­£ç­”ã—ãŸã‚±ãƒ¼ã‚¹ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå¿˜å´é˜²æ­¢ç”¨ãƒªãƒãƒ¼ã‚µãƒ«ãƒ‡ãƒ¼ã‚¿ï¼‰"""
        correct_samples = []
        
        for result in results:
            # overall_labelãŒcorrectã®ã‚‚ã®ã®ã¿æŠ½å‡º
            if result.overall_label == "correct":
                symptom = result.question
                
                # SAMPLE_MEDICAL_DATAã‹ã‚‰æ­£ã—ã„ç­”ãˆã‚’å–å¾—
                from data_generator import find_correct_medication_from_sample_data, format_instruction
                correct_medications = find_correct_medication_from_sample_data(symptom)
                
                if correct_medications:
                    instruction = format_instruction(symptom)
                    formatted_output = self._format_medications_from_json(correct_medications)
                    
                    correct_samples.append({
                        "instruction": instruction,
                        "input": "",
                        "output": formatted_output,
                        "symptom": symptom
                    })
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        import random
        if len(correct_samples) > num_samples:
            sampled = random.sample(correct_samples, num_samples)
        else:
            sampled = correct_samples
        
        print(f"  æ­£ç­”ã‚±ãƒ¼ã‚¹ã‹ã‚‰{len(sampled)}ä»¶ã‚’ãƒªãƒãƒ¼ã‚µãƒ«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æŠ½å‡ºï¼ˆå…¨{len(correct_samples)}ä»¶ä¸­ï¼‰")
        return sampled
    
    def create_dynamic_training_data(self, samples: List[Dict], iteration: int) -> str:
        """å‹•çš„ã«æŠ½å‡ºã—ãŸã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰æ•™å¸«ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        output_path = self.data_dir / f"training_data_iteration_{iteration}.jsonl"
        
        with open(output_path, "w", encoding="utf-8") as f:
            for sample in samples:
                f.write(json.dumps(sample, ensure_ascii=False) + "\n")
        
        print(f"  å‹•çš„æ•™å¸«ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {output_path} ({len(samples)}ä»¶)")
        return str(output_path)
    
    def create_curriculum_training_data(
        self, 
        prev_results: List,
        iteration: int
    ) -> str:
        """ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        
        èª¤ç­”ä¿®æ­£ãƒ‡ãƒ¼ã‚¿(A) + æˆåŠŸä¾‹ãƒªãƒãƒ¼ã‚µãƒ«ãƒ‡ãƒ¼ã‚¿(B)ã‚’çµ„ã¿åˆã‚ã›ã¾ã™ã€‚
        SAMPLE_MEDICAL_DATAã‹ã‚‰æ­£ã—ã„å‡¦æ–¹ã®ã¿ã‚’å–å¾—ã—ã¾ã™ã€‚
        
        Args:
            prev_results: å‰å›ã®è©•ä¾¡çµæœãƒªã‚¹ãƒˆ
            iteration: ç¾åœ¨ã®iterationç•ªå·
        
        Returns:
            ä½œæˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
        """
        # (A) èª¤ç­”ä¿®æ­£ãƒ‡ãƒ¼ã‚¿: é–“é•ãˆãŸã‚±ãƒ¼ã‚¹ã‚’SAMPLE_MEDICAL_DATAã®æ­£ã—ã„ç­”ãˆã§å­¦ç¿’
        incorrect_samples = self.extract_incorrect_cases(prev_results)
        
        # (B) æˆåŠŸä¾‹ãƒªãƒãƒ¼ã‚µãƒ«ãƒ‡ãƒ¼ã‚¿: æ­£ç­”ã—ãŸã‚±ãƒ¼ã‚¹ã‹ã‚‰ä¸€éƒ¨ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå¿˜å´é˜²æ­¢ï¼‰
        num_rehearsal = int(len(incorrect_samples) * self.rehearsal_ratio)
        rehearsal_samples = self.extract_correct_cases(prev_results, num_rehearsal)
        
        # A + B ã‚’çµåˆ
        
        # symptomãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‰Šé™¤ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã«ã™ã‚‹
        clean_incorrect = [{
            "instruction": s["instruction"],
            "input": s["input"],
            "output": s["output"]
        } for s in incorrect_samples]
        
        clean_rehearsal = [{
            "instruction": s["instruction"],
            "input": s["input"],
            "output": s["output"]
        } for s in rehearsal_samples]
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦çµåˆ
        final_samples = clean_incorrect + clean_rehearsal
        
        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        output_path = self.data_dir / f"training_data_iteration_{iteration}.jsonl"
        with open(output_path, "w", encoding="utf-8") as f:
            for sample in final_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + "\n")
        
        print(f"\n  ğŸ“Š ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†:")
        print(f"    - èª¤ç­”ä¿®æ­£ãƒ‡ãƒ¼ã‚¿ (A): {len(clean_incorrect)}ä»¶")
        print(f"    - æˆåŠŸä¾‹ãƒªãƒãƒ¼ã‚µãƒ«ãƒ‡ãƒ¼ã‚¿ (B): {len(clean_rehearsal)}ä»¶")
        print(f"    - åˆè¨ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(final_samples)}ä»¶")
        print(f"    - ä¿å­˜å…ˆ: {output_path}")
        
        return str(output_path)
    
    def run(self):
        """åå¾©å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ"""
        
        print("\n" + "=" * 70)
        print("ğŸš€ åŒ»ç™‚LLM LoRAåå¾©å­¦ç¿’ãƒ»è©•ä¾¡ãƒ«ãƒ¼ãƒ—")
        print("=" * 70)
        print(f"å®Ÿé¨“ID: {self.experiment_id}")
        print(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {self.base_model_name}")
        print(f"åå¾©å›æ•°: {self.num_iterations}")
        print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡: {'æœ‰åŠ¹' if self.enable_baseline else 'ç„¡åŠ¹ï¼ˆåˆå›å­¦ç¿’ã‹ã‚‰é–‹å§‹ï¼‰'}")
        print(f"å‡ºåŠ›å…ˆ: {self.output_dir}")
        print("=" * 70)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
        train_path, eval_path = self.setup_data()
        eval_data = load_evaluation_data(eval_path)
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if self.enable_baseline:
            print("\n" + "=" * 70)
            print("ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™")
            print("=" * 70)
            baseline_result = self.evaluate_model(
                model_name_or_path=self.base_model_name,
                eval_data=eval_data,
                iteration=0,
                is_baseline=True
            )
            self.iteration_results.append(baseline_result)
        else:
            print("\n" + "=" * 70)
            print("âš¡ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€åˆå›å­¦ç¿’ã‹ã‚‰é–‹å§‹ã—ã¾ã™")
            print("   ç†ç”±: Qwen 0.6Bã¯åŒ»ç™‚çŸ¥è­˜ãŒã»ã¼ã‚¼ãƒ­ã§ã€è©•ä¾¡ã‚³ã‚¹ãƒˆãŒç„¡é§„ã«ãªã‚‹ãŸã‚")
            print("=" * 70)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: åˆå›å¿…é ˆå­¦ç¿’ï¼ˆIteration 0ï¼‰
        print("\n" + "=" * 70)
        print("ğŸ¯ åˆå›å­¦ç¿’ï¼ˆIteration 0ï¼‰: training_data.jsonlã§äº‹å‰å­¦ç¿’")
        print("=" * 70)
        
        config = LoRATrainingConfig()
        trainer = MedicalLoRATrainer(config)
        trainer.setup_model(previous_checkpoint=None)  # åˆå›ã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰
        trainer.setup_lora()
        
        # å…ƒã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
        train_dataset = trainer.prepare_dataset(train_path)
        initial_checkpoint = trainer.train(train_dataset, iteration=0, previous_checkpoint=None)
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del trainer
        torch.cuda.empty_cache()
        
        # åˆå›ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
        print("\n" + "=" * 70)
        print("ğŸ“ˆ åˆå›å­¦ç¿’å¾Œã®è©•ä¾¡ï¼ˆIteration 0ï¼‰")
        print("=" * 70)
        initial_result = self.evaluate_model(
            model_name_or_path=initial_checkpoint,
            eval_data=eval_data,
            iteration=0,
            is_baseline=False
        )
        self.iteration_results.append(initial_result)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: åå¾©å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆå‹•çš„ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ + ç¶™ç¶šå­¦ç¿’ï¼‰
        previous_checkpoint = initial_checkpoint  # å‰å›ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿æŒ
        
        for iteration in range(1, self.num_iterations + 1):
            print("\n" + "=" * 70)
            print(f"ğŸ”„ åå¾© {iteration}/{self.num_iterations}")
            print(f"   å‰å›ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {previous_checkpoint}")
            print("=" * 70)
            
            # å‰å›ã®è©•ä¾¡çµæœã‹ã‚‰incorrect/unsafeã‚±ãƒ¼ã‚¹ã‚’æŠ½å‡º
            print(f"\nğŸ“Š Iteration {iteration-1}ã®è©•ä¾¡çµæœã‹ã‚‰èª¤ç­”ã‚±ãƒ¼ã‚¹ã‚’æŠ½å‡º...")
            prev_result_path = self.output_dir / f"iteration_{iteration-1}_evaluation.json"
            
            with open(prev_result_path, "r", encoding="utf-8") as f:
                prev_data = json.load(f)
            
            # EvaluationResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å†æ§‹ç¯‰
            from evaluator import EvaluationResult, DrugEvaluation
            prev_results = []
            for r in prev_data["results"]:
                drugs = [DrugEvaluation(**d) for d in r.get("drugs", [])]
                prev_results.append(EvaluationResult(
                    question=r["question"],
                    model_answer=r["model_answer"],
                    overall_label=r["overall_label"],
                    overall_is_harmful=r["overall_is_harmful"],
                    overall_score=r["overall_score"],
                    overall_reason=r["overall_reason"],
                    drugs=drugs,
                    timestamp=r["timestamp"],
                    expected_medications=r.get("expected_medications"),
                    correct_medications=r.get("correct_medications")  # ChatGPTã®æ­£ã—ã„è–¬å‰¤ãƒªã‚¹ãƒˆï¼ˆJSONé…åˆ—ï¼‰
                ))
            
            # ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            # èª¤ç­”ä¿®æ­£ãƒ‡ãƒ¼ã‚¿(A) + æˆåŠŸä¾‹ãƒªãƒãƒ¼ã‚µãƒ«ãƒ‡ãƒ¼ã‚¿(B)
            train_data_path = self.create_curriculum_training_data(
                prev_results=prev_results,
                iteration=iteration
            )
            
            # LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå‰å›ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ç¶™ç¶šï¼‰
            config = LoRATrainingConfig()
            trainer = MedicalLoRATrainer(config)
            
            # å‰å›ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰èª­ã¿è¾¼ã¿
            trainer.setup_model(previous_checkpoint=previous_checkpoint)
            trainer.setup_lora()
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
            train_dataset = trainer.prepare_dataset(train_data_path)
            
            # å­¦ç¿’å®Ÿè¡Œï¼ˆå‰å›ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ç¶™ç¶šå­¦ç¿’ï¼‰
            checkpoint_path = trainer.train(
                train_dataset,
                iteration=iteration,
                previous_checkpoint=previous_checkpoint
            )
            
            # æ¬¡ã®iterationã®ãŸã‚ã«æ›´æ–°
            previous_checkpoint = checkpoint_path
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del trainer
            torch.cuda.empty_cache()
            
            # è©•ä¾¡å®Ÿè¡Œ
            eval_result = self.evaluate_model(
                model_name_or_path=checkpoint_path,
                eval_data=eval_data,
                iteration=iteration,
                is_baseline=False
            )
            self.iteration_results.append(eval_result)
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_final_report()
    
    def generate_final_report(self):
        """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        
        print("\n" + "=" * 70)
        print("ğŸ“Š æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 70)
        
        # åå¾©ã”ã¨ã®ã‚¹ã‚³ã‚¢æ¨ç§»
        print("\nã€ç·åˆã‚¹ã‚³ã‚¢ãƒ»æ­£ç­”ç‡ãƒ»æœ‰å®³ç‡ã®æ¨ç§»ã€‘")
        print("-" * 90)
        print(f"{'Iteration':<15} {'ç·åˆã‚¹ã‚³ã‚¢':<20} {'æ­£ç­”ç‡':<20} {'æœ‰å®³ç‡':<20}")
        print("-" * 90)
        
        for result in self.iteration_results:
            if result["is_baseline"]:
                iteration_label = "Baseline"
            else:
                iteration_label = f"Iteration {result['iteration']}"
            
            metrics = result["metrics"]
            
            overall_mean = metrics['overall_score']['mean']
            overall_std = metrics['overall_score']['std']
            accuracy_rate = metrics.get('accuracy_rate', 0.0)
            harmful_rate = metrics.get('harmful_rate', 0.0)
            
            print(
                f"{iteration_label:<15} "
                f"{overall_mean:>6.3f} Â± {overall_std:<6.3f}     "
                f"{accuracy_rate:>6.1%}              "
                f"{harmful_rate:>6.1%}"
            )
        
        print("-" * 90)
        
        # ã‚¹ã‚³ã‚¢æ”¹å–„ã®è¨ˆç®—
        if len(self.iteration_results) > 1:
            initial_score = self.iteration_results[0]["metrics"]["overall_score"]["mean"]
            final_score = self.iteration_results[-1]["metrics"]["overall_score"]["mean"]
            improvement = final_score - initial_score
            improvement_pct = (improvement / initial_score) * 100 if initial_score > 0 else 0
            
            initial_harmful = self.iteration_results[0]["metrics"].get("harmful_rate", 0.0)
            final_harmful = self.iteration_results[-1]["metrics"].get("harmful_rate", 0.0)
            harmful_reduction = initial_harmful - final_harmful
            
            initial_label = "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³" if self.iteration_results[0]["is_baseline"] else "åˆå›å­¦ç¿’å¾Œ"
            
            print("\nã€æ”¹å–„åº¦ã€‘")
            print(f"  {initial_label}ç·åˆã‚¹ã‚³ã‚¢: {initial_score:.3f}")
            print(f"  æœ€çµ‚ç·åˆã‚¹ã‚³ã‚¢:         {final_score:.3f}")
            print(f"  æ”¹å–„:                   {improvement:+.3f} ({improvement_pct:+.1f}%)")
            print(f"\n  {initial_label}æœ‰å®³ç‡:     {initial_harmful:.1%}")
            print(f"  æœ€çµ‚æœ‰å®³ç‡:             {final_harmful:.1%}")
            print(f"  å‰Šæ¸›:                   {harmful_reduction:+.1%}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        report_path = self.output_dir / f"experiment_{self.experiment_id}_report.json"
        report_data = {
            "experiment_id": self.experiment_id,
            "base_model": self.base_model_name,
            "num_iterations": self.num_iterations,
            "results": self.iteration_results,
            "timestamp": datetime.now(timezone(timedelta(hours=9))).isoformat()
        }
        
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        print("=" * 70)


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    # .envã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
    base_model = os.getenv("BASE_MODEL", "Qwen/Qwen3-0.6B")
    iterations = int(os.getenv("ITERATIONS", "2"))
    output_dir = os.getenv("OUTPUT_DIR", "results")
    num_train_samples = int(os.getenv("NUM_TRAIN_SAMPLES", "50"))
    num_eval_samples = int(os.getenv("NUM_EVAL_SAMPLES", "15"))  # ä»•æ§˜æ›¸ã®15ä»¶ã«å¤‰æ›´
    num_samples_per_question = int(os.getenv("NUM_SAMPLES_PER_QUESTION", "30"))  # Nå›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    enable_baseline = os.getenv("ENABLE_BASELINE", "false").lower() == "true"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: false
    rehearsal_ratio = float(os.getenv("REHEARSAL_RATIO", "0.5"))  # èª¤ç­”æ•°ã«å¯¾ã™ã‚‹æˆåŠŸä¾‹ãƒªãƒãƒ¼ã‚µãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡
    
    print("\n" + "=" * 70)
    print("ğŸ“‹ è¨­å®šæƒ…å ± (.envã‹ã‚‰èª­ã¿è¾¼ã¿)")
    print("=" * 70)
    print(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {base_model}")
    print(f"åå¾©å›æ•°: {iterations}")
    print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print(f"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«æ•°: {num_train_samples}")
    print(f"è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°: {num_eval_samples}")
    print(f"è³ªå•ã‚ãŸã‚Šã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°: {num_samples_per_question}")
    print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡: {'æœ‰åŠ¹' if enable_baseline else 'ç„¡åŠ¹'}")
    print(f"ãƒªãƒãƒ¼ã‚µãƒ«ãƒ‡ãƒ¼ã‚¿æ¯”ç‡: {rehearsal_ratio}x (èª¤ç­”æ•°ã«å¯¾ã™ã‚‹å€ç‡)")
    print("=" * 70)
    
    # ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ
    loop = IterativeTrainingLoop(
        base_model_name=base_model,
        num_iterations=iterations,
        output_dir=output_dir,
        num_samples_per_question=num_samples_per_question,
        enable_baseline=enable_baseline,
        rehearsal_ratio=rehearsal_ratio
    )
    
    loop.run()
    
    print("\nâœ… å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ!")


if __name__ == "__main__":
    main()
